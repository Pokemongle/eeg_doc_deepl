{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9c18cb9",
   "metadata": {},
   "source": [
    "# Head files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d6168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from load_data import MyData  # self-made\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm_notebook as tqdm # View procedure\n",
    "import os\n",
    "import scipy.io\n",
    "from random import random\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from net_cnn_lstm6 import MyNetwork\n",
    "from torchnlp.word_to_vector import GloVe\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1bcff",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102683c3",
   "metadata": {},
   "source": [
    "## Set arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d975fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "C,H,W = 1,1,2400\n",
    "# learn_rate = 0.0005\n",
    "learn_rate=0.001\n",
    "# num_epochs=50\n",
    "num_epochs = 70\n",
    "\n",
    "# ================================================ random\n",
    "BATCH_SIZE = 1\n",
    "C,H,W = 1,1,2400\n",
    "# learn_rate = 0.0005\n",
    "learn_rate=0.001\n",
    "# num_epochs=50\n",
    "num_epochs = 70\n",
    "\n",
    "# ================================================ optimizer\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda:0\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# ==损失函数权重\n",
    "# ======== 二分类HC/DOC\n",
    "# 计算总样本数量\n",
    "# condition1\n",
    "# total_samples = 887 + 985 + 879\n",
    "# condition2\n",
    "# total_samples = 929 + 1029 + 886\n",
    "# condition3\n",
    "# total_samples = 887 + 975 + 879\n",
    "# rest\n",
    "total_samples = 852 + 1051 + 872\n",
    "# 计算每个类别的权重\n",
    "# condition1\n",
    "# weights = [total_samples / 887, total_samples / (985 + 879)]\n",
    "# condition2\n",
    "# weights = [total_samples / 929, total_samples / (1029 + 886)]\n",
    "# condition3\n",
    "weights = [total_samples / 852, total_samples / (1051 + 872)]\n",
    "\n",
    "# ======== 二分类MCS/UWS\n",
    "# 计算总样本数量\n",
    "# condition1\n",
    "# total_samples = 985 + 879\n",
    "# condition2\n",
    "# total_samples = 1029 + 886\n",
    "# condition3\n",
    "# total_samples = 975 + 879\n",
    "# rest\n",
    "# total_samples = 1051 + 872\n",
    "# 计算每个类别的权重\n",
    "# condition1\n",
    "# weights = [total_samples / 985, total_samples / 879]\n",
    "# condition2\n",
    "# weights = [total_samples / 1029, total_samples / 886]\n",
    "# condition3\n",
    "# weights = [total_samples / 975, total_samples / 879]\n",
    "# condition3\n",
    "# weights = [total_samples / 1051, total_samples / 872]\n",
    "# 将权重转换为张量\n",
    "weights_tensor = torch.tensor(weights, device=device)\n",
    "\n",
    "# 定义交叉熵损失函数并设置权重\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7368b",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc17de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(dataset,mode):\n",
    "    # find the fold file\n",
    "    count = 0\n",
    "    for person in range(len(dataset)):\n",
    "        filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "        # extract the pure name of the file\n",
    "        parts = filename.split(\"\\\\\")\n",
    "        file_name = parts[-1]\n",
    "        name_without_extension = file_name.split(\".\")[0]\n",
    "        # label or data\n",
    "        file_last = name_without_extension.split(\"_\")[-1]\n",
    "        if file_last.isdigit(): # data\n",
    "            # is this fold or not\n",
    "            if int(file_last) == fold: # yes\n",
    "                print(filename)\n",
    "                count = count + 1\n",
    "                data_map = torch.load(filename)\n",
    "                # train or test\n",
    "                if name_without_extension.split(\"_\")[-2] == \"train\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        train_data.append(data_map[i])\n",
    "                elif name_without_extension.split(\"_\")[-2] == \"test\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        test_data.append(data_map[i])\n",
    "                if count == 4:\n",
    "                    del data_map\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache() \n",
    "                    break\n",
    "            else:   # not\n",
    "                pass\n",
    "        else: # label\n",
    "            file_last = name_without_extension.split(\"_\")[-4]\n",
    "            file_mode = name_without_extension.split(\"_\")[-2]\n",
    "            if file_mode == mode.split(\"_\")[-1]: # is this mode or not\n",
    "                # is this fold or not\n",
    "                if int(file_last) == fold: # yes\n",
    "                    print(filename)\n",
    "                    count = count + 1\n",
    "                    data_map = torch.load(filename)\n",
    "                    # train or test\n",
    "                    if name_without_extension.split(\"_\")[-5] == \"train\":\n",
    "                        for i in range(data_map.size(0)):\n",
    "                            train_label.append(data_map[i])\n",
    "                    elif name_without_extension.split(\"_\")[-5] == \"test\":\n",
    "                        for i in range(data_map.size(0)):\n",
    "                            test_label.append(data_map[i])\n",
    "                    if count == 4:\n",
    "                        del data_map\n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache() \n",
    "                        break\n",
    "                else:   # not\n",
    "                    pass\n",
    "            else:\n",
    "                pass\n",
    "        del filename, parts, file_name, name_without_extension, file_last\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()    \n",
    "# 定义LSTM超参数\n",
    "input_size = 64  # 输入特征维度\n",
    "hidden_size = 64  # 隐藏单元数量\n",
    "num_layers = 2  # LSTM层数\n",
    "output_size = 2  # 输出类别数量\n",
    "# 创建模型实例\n",
    "# model_list = ['', '_CNN', '_CNN_spa', '_CNN_spa_lstm']\n",
    "# model_name = model_list[0]\n",
    "# if model_name == model_list[0]: #  CascadeCept\n",
    "#     from network_cnn_lstm import MyNetwork\n",
    "# elif model_name == model_list[1]: #  CNN\n",
    "#     from network_cnn_lstm_2 import MyNetwork\n",
    "# elif model_name == model_list[2]: # CascadeCept_1\n",
    "#     from network_cnn_lstm_3 import MyNetwork\n",
    "# elif model_name == model_list[3]: # CascadeCept_2\n",
    "#     from network_cnn_lstm_4 import MyNetwork\n",
    "model_list = ['_1', '_2', '_3', '_4','_5','_6','_7','_8','_9','_10','_11']\n",
    "model_name = model_list[5]\n",
    "if model_name == model_list[0]: \n",
    "    from net_cnn_lstm1 import MyNetwork\n",
    "elif model_name == model_list[1]: \n",
    "    from net_cnn_lstm2 import MyNetwork\n",
    "elif model_name == model_list[2]: \n",
    "    from net_cnn_lstm3 import MyNetwork\n",
    "elif model_name == model_list[3]: \n",
    "    from net_cnn_lstm4 import MyNetwork\n",
    "elif model_name == model_list[4]: \n",
    "    from net_cnn_lstm5 import MyNetwork\n",
    "elif model_name == model_list[5]: \n",
    "    from net_cnn_lstm6 import MyNetwork\n",
    "elif model_name == model_list[6]: \n",
    "    from net_cnn_lstm7 import MyNetwork\n",
    "elif model_name == model_list[7]: \n",
    "    from net_cnn_lstm8 import MyNetwork\n",
    "elif model_name == model_list[8]: \n",
    "    from net_cnn_lstm9 import MyNetwork\n",
    "elif model_name == model_list[9]: \n",
    "    from net_cnn_lstm10 import MyNetwork\n",
    "elif model_name == model_list[10]: \n",
    "    from net_cnn_lstm11 import MyNetwork\n",
    "model = MyNetwork(input_size, hidden_size, num_layers, output_size)\n",
    "model = model.to(device)\n",
    "\n",
    "# experimental dir: rest, conditionA, conditionB, conditionC\n",
    "exper_dir = \"rest59\"\n",
    "trial=\"_3\"\n",
    "# root_dir = f\"../data/eegmap_split/{exper_dir}\"\n",
    "root_dir = f\"../data/eegmap_split/{exper_dir}\"\n",
    "# classification = \"hc_doc\"/doc or \"mcs_uws\"/uws\n",
    "classification = \"hc_doc\"\n",
    "# classification = \"mcs_uws\"\n",
    "fold_num = 5\n",
    "best_val_loss = float('inf')\n",
    "for fold in tqdm(range(5)):\n",
    "    # train num folds\n",
    "#     fold = 0 # 选择折数\n",
    "    # -- prepare datasets\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "    \n",
    "    if classification == \"hc_doc\":\n",
    "        # ---- hc\n",
    "        dataset = MyData(root_dir, \"train\", \"hc\") # hc\n",
    "        make_dataset(dataset,classification)\n",
    "        dataset = MyData(root_dir, \"test\", \"hc\") # hc\n",
    "        make_dataset(dataset,classification)\n",
    "    # ---- mcs\n",
    "    dataset = MyData(root_dir, \"train\", \"mcs\") # mcs\n",
    "    make_dataset(dataset,classification)\n",
    "    dataset = MyData(root_dir, \"test\", \"mcs\") # hc\n",
    "    make_dataset(dataset,classification)\n",
    "    # ---- uws\n",
    "    dataset = MyData(root_dir, \"train\", \"uws\") # uws\n",
    "    make_dataset(dataset,classification)\n",
    "    dataset = MyData(root_dir, \"test\", \"uws\") # hc\n",
    "    make_dataset(dataset,classification)\n",
    "    \n",
    "    print(torch.stack(train_data).size())\n",
    "    print(torch.stack(train_label).size())\n",
    "    print(torch.stack(test_data).size())\n",
    "    print(torch.stack(test_label).size())\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()  \n",
    "    \n",
    "    train_data = torch.stack(train_data)\n",
    "    train_label = torch.stack(train_label)\n",
    "    test_data = torch.stack(test_data)\n",
    "    test_label = torch.stack(test_label)\n",
    "    # train dataset\n",
    "    train_td = TensorDataset(train_data, train_label)\n",
    "    train_loader = DataLoader(train_td, batch_size = BATCH_SIZE, shuffle = True)\n",
    "    # test dataset\n",
    "    test_td = TensorDataset(test_data, test_label)\n",
    "    test_loader = DataLoader(test_td, batch_size = BATCH_SIZE, shuffle = True)\n",
    "    del train_data, train_label, test_data, test_label, train_td, test_td\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # set model for each fold\n",
    "    model = MyNetwork(input_size, hidden_size, num_layers, output_size)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learn_rate)\n",
    "#     scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 30, 50], gamma=0.2)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    # -- start training\n",
    "    start_time = time.time()\n",
    "    # train and test step records\n",
    "    total_train_step = 0\n",
    "    total_test_step = 0\n",
    "    min_test_loss = 1000\n",
    "    # add Tensorboard\n",
    "#     writer_train = SummaryWriter(f\"../logs/{classification}/{exper_dir}{model_name}/logs_train_{fold}\")\n",
    "#     writer_valid = SummaryWriter(f\"../logs/{classification}/{exper_dir}{model_name}/logs_test_{fold}\")\n",
    "#     writer_valid_acc = SummaryWriter(f\"../logs/{classification}/{exper_dir}{model_name}/logs_test_acc_{fold}\")\n",
    "    writer_train = SummaryWriter(f\"../logs/{classification}/{exper_dir}{model_name}{trial}/logs_train_{fold}\")\n",
    "    writer_valid = SummaryWriter(f\"../logs/{classification}/{exper_dir}{model_name}{trial}/logs_test_{fold}\")\n",
    "    writer_valid_acc = SummaryWriter(f\"../logs/{classification}/{exper_dir}{model_name}{trial}/logs_test_acc_{fold}\")\n",
    "    for i in tqdm(range(num_epochs)):  \n",
    "        print(f\"========= Epoch {i} Training =========\")\n",
    "        # train steps\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            # x, y\n",
    "            data_map, label=data\n",
    "#             data_map_reshaped = torch.reshape(data_map, (110, 1, 1, 2400))\n",
    "            data_map_reshaped = torch.reshape(data_map, (59, 1, 1, 2400))\n",
    "            label_int = label.long()\n",
    "            data_map_reshaped=data_map_reshaped.to(device)\n",
    "            label_int=label_int.to(device)\n",
    "            del data_map, label\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            # y_pred\n",
    "            label_pred = model(data_map_reshaped)\n",
    "            # Loss Computation and Optimization\n",
    "            loss = criterion(label_pred,label_int)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # draw tensorboard\n",
    "            total_train_step = total_train_step + 1\n",
    "            # print info\n",
    "            if total_train_step % 500 == 0:\n",
    "                end_time = time.time()\n",
    "                print(label_pred)\n",
    "                print(f\"Train time: {end_time - start_time}\")\n",
    "                print(f\"Train steps: {total_train_step}, Loss: {loss.item()}\")\n",
    "            writer_train.add_scalar(\"train_loss\",loss.item(),total_train_step)\n",
    "            # Clear gpu\n",
    "            del data, data_map_reshaped, label_int, label_pred, loss\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Evaluation and save the best model\n",
    "        print(f\"========= Epoch {i} Testing =========\")\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        total_test_loss = 0\n",
    "        test_count = 0\n",
    "        total_test_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                test_count = test_count + 1\n",
    "                # x, y\n",
    "                data_map, label=data\n",
    "#                 data_map_reshaped = torch.reshape(data_map, (110, 1, 1, 2400))\n",
    "                data_map_reshaped = torch.reshape(data_map, (59, 1, 1, 2400))\n",
    "                label_int = label.long()\n",
    "                data_map_reshaped = data_map_reshaped.to(device)\n",
    "                label_int = label_int.to(device)\n",
    "                del data_map, label\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                # y_pred\n",
    "                label_pred_test = model(data_map_reshaped)\n",
    "                loss = criterion(label_pred_test,label_int)\n",
    "#                 print(label_pred_test)\n",
    "                # accuracy \n",
    "                total_test_acc = total_test_acc + ((label_pred_test.argmax(1)) == label_int).sum()\n",
    "                # change lr\n",
    "                test_loss += loss.item()\n",
    "                test_loss /= len(test_loader)\n",
    "                if test_loss < best_val_loss:\n",
    "                    best_val_loss = test_loss\n",
    "                scheduler.step(test_loss)\n",
    "                # draw tensorboad\n",
    "                total_test_loss = total_test_loss + loss\n",
    "                if test_count % 100 == 0:\n",
    "                    print(f\"Loss: {total_test_loss} Accuracy: {total_test_acc/test_count}\")\n",
    "                # Clear gpu\n",
    "                del data_map_reshaped, label_int, label_pred_test, loss, data\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "        print(f\"Total Loss: {total_test_loss} Total Accuracy: {total_test_acc/test_count}\")\n",
    "        writer_valid.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n",
    "        writer_valid_acc.add_scalar(\"test_acc\", total_test_acc/test_count, total_test_step)\n",
    "        total_test_step = total_test_step + 1\n",
    "        print(\"..........Saving the model..........\")\n",
    "#         torch.save(model.state_dict(),f\"../models/{classification}/{exper_dir}{model_name}/Fold{fold}_Epoch{i}.pt\") \n",
    "        torch.save(model.state_dict(),f\"../models/{classification}/{exper_dir}{model_name}{trial}/Fold{fold}_Epoch{i}.pt\") \n",
    "#         if total_test_loss < min_test_loss:\n",
    "#             min_test_loss = total_test_loss\n",
    "#             print(\"..........Saving the model..........\")\n",
    "#             torch.save(model.state_dict(),f\"../model/{exper_dir}/Fold{fold}_Epoch{i}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
