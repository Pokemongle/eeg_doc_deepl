{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2279a98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from load_data import MyData  # self-made\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm_notebook as tqdm # View procedure\n",
    "import os\n",
    "import scipy.io\n",
    "from random import random\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from network_cnn_lstm_4 import MyNetwork\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c5ac591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_map size: torch.Size([135, 2400, 10, 11])\n",
      "data_map size: torch.Size([135])\n",
      "data_map size: torch.Size([122, 2400, 10, 11])\n",
      "data_map size: torch.Size([122])\n",
      "torch.Size([257, 2400, 10, 11])\n",
      "torch.Size([257])\n"
     ]
    }
   ],
   "source": [
    "# exper_dir = \"rest\"\n",
    "exper_dir = \"conditionC\"\n",
    "root_dir = f\"../data/eegmap_split/{exper_dir}\"\n",
    "test_data = []\n",
    "test_label = []\n",
    "# # ---- hc\n",
    "# dataset = MyData(root_dir, f\"test\", \"hc\") # hc\n",
    "# # find the fold file\n",
    "# for person in range(len(dataset)):\n",
    "#     filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "#     data_map = torch.load(filename)\n",
    "#     print(f\"data_map size: {data_map.size()}\")\n",
    "#     # extract the pure name of the file\n",
    "#     parts = filename.split(\"\\\\\")\n",
    "#     file_name = parts[-1]\n",
    "#     name_without_extension = file_name.split(\".\")[0]\n",
    "#     # label or data\n",
    "#     file_last = name_without_extension.split(\"_\")[-1]\n",
    "#     if file_last == \"label\": # label\n",
    "#         for label in data_map:\n",
    "#             test_label.append(label)\n",
    "#     else: # data\n",
    "#         for data in data_map:\n",
    "#             test_data.append(data)\n",
    "#     del filename, parts, name_without_extension, file_last\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()   \n",
    "# ---- mcs\n",
    "dataset = MyData(root_dir, \"test\", \"mcs\") \n",
    "# find the fold file\n",
    "for person in range(len(dataset)):\n",
    "    filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "    data_map = torch.load(filename)\n",
    "    print(f\"data_map size: {data_map.size()}\")\n",
    "    # extract the pure name of the file\n",
    "    parts = filename.split(\"\\\\\")\n",
    "    file_name = parts[-1]\n",
    "    name_without_extension = file_name.split(\".\")[0]\n",
    "    # label or data\n",
    "    file_last = name_without_extension.split(\"_\")[-1]\n",
    "    if file_last == \"label\": # label\n",
    "        for label in data_map:\n",
    "            test_label.append(label)\n",
    "    else: # data\n",
    "        for data in data_map:\n",
    "            test_data.append(data)\n",
    "    del filename, parts, name_without_extension, file_last\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()  \n",
    "# ---- uws\n",
    "dataset = MyData(root_dir, \"test\", \"uws\") # uws\n",
    "# find the fold file\n",
    "for person in range(len(dataset)):\n",
    "    filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "    data_map = torch.load(filename)\n",
    "    print(f\"data_map size: {data_map.size()}\")\n",
    "    # extract the pure name of the file\n",
    "    parts = filename.split(\"\\\\\")\n",
    "    file_name = parts[-1]\n",
    "    name_without_extension = file_name.split(\".\")[0]\n",
    "    # label or data\n",
    "    file_last = name_without_extension.split(\"_\")[-1]\n",
    "    if file_last == \"label\": # label\n",
    "        for label in data_map:\n",
    "            test_label.append(label)\n",
    "    else: # data\n",
    "        for data in data_map:\n",
    "            test_data.append(data)\n",
    "    del filename, parts, name_without_extension, file_last\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()  \n",
    "test_data = torch.stack(test_data)\n",
    "test_label = torch.stack(test_label)\n",
    "print(test_data.size())\n",
    "print(test_label.size())\n",
    "test_data_size = test_label.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e8bfac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "C,H,W = 1,1,2400\n",
    "learn_rate = 0.0005\n",
    "num_epochs = 100\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# test dataset\n",
    "test_td = TensorDataset(test_data, test_label)\n",
    "test_loader = DataLoader(test_td, batch_size = BATCH_SIZE, shuffle = True)\n",
    "del test_data\n",
    "del test_label\n",
    "del test_td\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3702f83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/mcs_uws/conditionC_CNN_spa_lstm/Fold1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:75: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c5a176a3ba4a71a134ef3808aedf20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================Epoch 3 == Loss:100.10593042813707===============\n",
      "--Accuracy: 0.8015564202334631\n",
      "--Precision: 0.808695652173913\n",
      "--Recall: 0.7622950819672131\n",
      "=====================Epoch 4 == Loss:79.61288068778968===============\n",
      "--Accuracy: 0.8910505836575876\n",
      "--Precision: 0.821917808219178\n",
      "--Recall: 0.9836065573770492\n",
      "=====================Epoch 5 == Loss:96.65383107449765===============\n",
      "--Accuracy: 0.8482490272373541\n",
      "--Precision: 0.9662921348314607\n",
      "--Recall: 0.7049180327868853\n",
      "=====================Epoch 6 == Loss:29.640537146056246===============\n",
      "--Accuracy: 0.953307392996109\n",
      "--Precision: 0.9741379310344828\n",
      "--Recall: 0.9262295081967213\n",
      "=====================Epoch 7 == Loss:107.32997982983159===============\n",
      "--Accuracy: 0.8482490272373541\n",
      "--Precision: 0.816793893129771\n",
      "--Recall: 0.8770491803278688\n",
      "=====================Epoch 8 == Loss:32.82107461770293===============\n",
      "--Accuracy: 0.9494163424124513\n",
      "--Precision: 1.0\n",
      "--Recall: 0.8934426229508197\n",
      "=====================Epoch 9 == Loss:63.208351745223794===============\n",
      "--Accuracy: 0.9221789883268483\n",
      "--Precision: 0.8695652173913043\n",
      "--Recall: 0.9836065573770492\n",
      "=====================Epoch 10 == Loss:13.512599876952955===============\n",
      "--Accuracy: 0.9766536964980544\n",
      "--Precision: 0.967741935483871\n",
      "--Recall: 0.9836065573770492\n",
      "=====================Epoch 11 == Loss:12.980866433613578===============\n",
      "--Accuracy: 0.9766536964980544\n",
      "--Precision: 0.9833333333333333\n",
      "--Recall: 0.9672131147540983\n",
      "=====================Epoch 12 == Loss:75.96983318096687===============\n",
      "--Accuracy: 0.9027237354085603\n",
      "--Precision: 0.98989898989899\n",
      "--Recall: 0.8032786885245902\n",
      "=====================Epoch 13 == Loss:43.60615159517701===============\n",
      "--Accuracy: 0.9260700389105059\n",
      "--Precision: 0.9327731092436975\n",
      "--Recall: 0.9098360655737705\n",
      "=====================Epoch 14 == Loss:73.80191338022429===============\n",
      "--Accuracy: 0.914396887159533\n",
      "--Precision: 0.9385964912280702\n",
      "--Recall: 0.8770491803278688\n",
      "=====================Epoch 15 == Loss:31.9838805714816===============\n",
      "--Accuracy: 0.9688715953307393\n",
      "--Precision: 0.9596774193548387\n",
      "--Recall: 0.9754098360655737\n",
      "=====================Epoch 16 == Loss:32.706164486837935===============\n",
      "--Accuracy: 0.9455252918287937\n",
      "--Precision: 0.990909090909091\n",
      "--Recall: 0.8934426229508197\n",
      "=====================Epoch 17 == Loss:14.186274199527155===============\n",
      "--Accuracy: 0.980544747081712\n",
      "--Precision: 0.968\n",
      "--Recall: 0.9918032786885246\n",
      "=====================Epoch 18 == Loss:26.102759655509907===============\n",
      "--Accuracy: 0.9494163424124513\n",
      "--Precision: 0.990990990990991\n",
      "--Recall: 0.9016393442622951\n",
      "=====================Epoch 19 == Loss:42.128815041267266===============\n",
      "--Accuracy: 0.9299610894941635\n",
      "--Precision: 0.9905660377358491\n",
      "--Recall: 0.860655737704918\n",
      "=====================Epoch 20 == Loss:72.12956267261313===============\n",
      "--Accuracy: 0.8988326848249028\n",
      "--Precision: 0.98\n",
      "--Recall: 0.8032786885245902\n",
      "=====================Epoch 21 == Loss:42.607429905373415===============\n",
      "--Accuracy: 0.9494163424124513\n",
      "--Precision: 0.9739130434782609\n",
      "--Recall: 0.9180327868852459\n",
      "=====================Epoch 22 == Loss:57.468130339080886===============\n",
      "--Accuracy: 0.933852140077821\n",
      "--Precision: 0.972972972972973\n",
      "--Recall: 0.8852459016393442\n",
      "=====================Epoch 23 == Loss:74.46716576911994===============\n",
      "--Accuracy: 0.8443579766536965\n",
      "--Precision: 0.8306451612903226\n",
      "--Recall: 0.8442622950819673\n",
      "=====================Epoch 24 == Loss:59.741949128720556===============\n",
      "--Accuracy: 0.9182879377431906\n",
      "--Precision: 0.9809523809523809\n",
      "--Recall: 0.8442622950819673\n",
      "=====================Epoch 25 == Loss:44.21199509522097===============\n",
      "--Accuracy: 0.9027237354085603\n",
      "--Precision: 1.0\n",
      "--Recall: 0.7950819672131147\n",
      "=====================Epoch 26 == Loss:19.321438326265927===============\n",
      "--Accuracy: 0.9571984435797666\n",
      "--Precision: 0.9826086956521739\n",
      "--Recall: 0.9262295081967213\n",
      "=====================Epoch 27 == Loss:31.092445878605773===============\n",
      "--Accuracy: 0.9416342412451362\n",
      "--Precision: 1.0\n",
      "--Recall: 0.8770491803278688\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../model/mcs_uws/conditionC_CNN_spa_lstm/Fold1_Epoch28.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-aa3788b5327f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"../model/{classification}/{exper_dir}{model_name}/Fold{fold}_Epoch{i}.pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# 测试步骤开始\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../model/mcs_uws/conditionC_CNN_spa_lstm/Fold1_Epoch28.pt'"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda:0\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# 加载预训练模型!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# 定义LSTM超参数\n",
    "input_size = 64  # 输入特征维度\n",
    "hidden_size = 64  # 隐藏单元数量\n",
    "num_layers = 2  # LSTM层数\n",
    "output_size = 2  # 输出类别数量\n",
    "\n",
    "# ==损失函数权重\n",
    "# ======== 二分类-HC/DOC\n",
    "# 计算总样本数量\n",
    "# condition1\n",
    "# total_samples = 887 + 985 + 879\n",
    "# condition2\n",
    "# total_samples = 929 + 1029 + 886\n",
    "# condition3\n",
    "# total_samples = 887 + 975 + 879\n",
    "# 计算每个类别的权重\n",
    "# condition1\n",
    "# weights = [total_samples / 887, total_samples / (985 + 879)]\n",
    "# condition2\n",
    "# weights = [total_samples / 929, total_samples / (1029 + 886)]\n",
    "# condition3\n",
    "# weights = [total_samples / 887, total_samples / (975 + 879)]\n",
    "# ======== 二分类-MCS/UWS\n",
    "# 计算总样本数量\n",
    "# condition1\n",
    "# total_samples =985 + 879\n",
    "# condition2\n",
    "# total_samples = 1029 + 886\n",
    "# condition3\n",
    "# total_samples = 975 + 879\n",
    "# 计算每个类别的权重\n",
    "# condition1\n",
    "# weights = [total_samples / 985, total_samples / 879]\n",
    "# condition2\n",
    "# weights = [total_samples / 1029, total_samples / 886]\n",
    "# condition3\n",
    "# weights = [total_samples / 975, total_samples / 879]\n",
    "# # ======== 三分类\n",
    "# # 计算总样本数量\n",
    "# # conditionB\n",
    "# total_samples = 929 + 1029 + 886\n",
    "# # 计算每个类别的权重\n",
    "# # conditionB\n",
    "# weights = [total_samples / 929, total_samples / 1029, total_samples / 886]\n",
    "\n",
    "# condition3-HC/DOC\n",
    "# total_samples = 887 + 975 + 879\n",
    "# weights = [total_samples / 887, total_samples / (975 + 879)]\n",
    "# condition3-MCS/UWS\n",
    "total_samples = 975 + 879\n",
    "weights = [total_samples / 975, total_samples / 879]\n",
    "# 将权重转换为张量\n",
    "weights_tensor = torch.tensor(weights, device=device)\n",
    "# 定义交叉熵损失函数并设置权重\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "criterion = criterion.to(device)\n",
    "# 将权重转换为张量\n",
    "weights_tensor = torch.tensor(weights, device=device)\n",
    "# 定义交叉熵损失函数并设置权重\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "criterion = criterion.to(device)\n",
    "from network_cnn_lstm_4 import MyNetwork\n",
    "fold = 1\n",
    "begin = 3\n",
    "end = 80\n",
    "exper_dir = \"conditionC\"\n",
    "classification = \"mcs_uws\"\n",
    "model_name = \"_CNN_spa_lstm\"\n",
    "print(f\"../model/{classification}/{exper_dir}{model_name}/Fold{fold}.pt\")\n",
    "for i in tqdm(range(begin,end)):\n",
    "    model = MyNetwork(input_size, hidden_size, num_layers, output_size)\n",
    "    model.load_state_dict(torch.load(f\"../model/{classification}/{exper_dir}{model_name}/Fold{fold}_Epoch{i}.pt\"))\n",
    "    model = model.to(device)\n",
    "    # 测试步骤开始\n",
    "    model.eval()\n",
    "    # 初始化损失和准确率\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    total_test_loss = 0\n",
    "    # 计数\n",
    "    test_count = 0\n",
    "    # 用于绘制混淆矩阵\n",
    "    predicted_labels = torch.tensor([])\n",
    "    predicted_labels = predicted_labels.to(device)\n",
    "    true_labels = torch.tensor([])\n",
    "    true_labels = true_labels.to(device)\n",
    "    with torch.no_grad(): # 设置不进行后向传播\n",
    "        for data in test_loader:\n",
    "            test_count = test_count + 1\n",
    "            data_map, label=data # x,y\n",
    "            # x\n",
    "            data_map_reshaped = torch.reshape(data_map, (110, 1, 1, 2400))\n",
    "            data_map_reshaped = data_map_reshaped.to(device)\n",
    "            # y\n",
    "            label_int = label.long()\n",
    "            label_int = label_int.to(device)\n",
    "            # clear gpu\n",
    "            del data_map\n",
    "            del label\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            # y_pred\n",
    "            label_pred_test = model(data_map_reshaped)\n",
    "\n",
    "            # confusion matrix\n",
    "            predicted_labels = torch.cat((predicted_labels, label_pred_test.argmax(1)), dim=0)\n",
    "            true_labels = torch.cat((true_labels, label_int), dim=0)\n",
    "\n",
    "            # loss\n",
    "            loss = criterion(label_pred_test,label_int)\n",
    "            total_test_loss = total_test_loss + loss.item()\n",
    "\n",
    "            pred_label = ((label_pred_test.argmax(1)) == label_int).sum()\n",
    "            # calculation\n",
    "            if label_int == 1: # DOC\n",
    "                if pred_label == 1: # 预测对了\n",
    "                    TP = TP + 1\n",
    "                elif pred_label ==0: # 预测错了\n",
    "                    FN = FN + 1\n",
    "            if label_int == 0: # HC\n",
    "                if pred_label == 1: # 预测对了\n",
    "                    TN = TN + 1\n",
    "                elif pred_label == 0: # 预测错了\n",
    "                    FP = FP + 1\n",
    "    print(f\"=====================Epoch {i} == Loss:{total_test_loss}===============\")\n",
    "    print(f\"--Accuracy: {(TP + TN) / (TP + TN + FP + FN)}\")\n",
    "    print(f\"--Precision: {TP / (TP + FP)}\")\n",
    "    print(f\"--Recall: {TP / (TP + FN)}\")\n",
    "    del model,TP,TN,FP,FN,total_test_loss,test_count\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b2f27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
